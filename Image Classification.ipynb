{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "import sys\n", "import time\n", "import shutil\n", "import GPUtil\n", "import logging\n", "import skimage.io\n", "import numpy as np\n", "import pandas as pd\n", "import multiprocessing\n", "import tensorflow as tf\n", "from typing import Dict\n", "import dask.dataframe as dd\n", "from typing import Dict, Generator\n", "from typing import Dict, List, Generator\n", "from tensorflow.python.training.tracking.base import Trackable\n", "\n", "from perceptilabs.core_new.graph import Graph\n", "from perceptilabs.core_new.utils import Picklable\n", "from perceptilabs.core_new.layers.base import Tf1xLayer\n", "from perceptilabs.core_new.layers.base import DataSupervised\n", "from perceptilabs.core_new.utils import Picklable, YieldLevel\n", "from perceptilabs.core_new.communication import TrainingServer\n", "from perceptilabs.core_new.serialization import can_serialize, serialize\n", "from perceptilabs.core_new.layers.base import ClassificationLayer, Tf1xLayer\n", "from perceptilabs.core_new.graph.builder import GraphBuilder, SnapshotBuilder\n", "from perceptilabs.messaging import ZmqMessagingFactory, SimpleMessagingFactory\n", "from perceptilabs.core_new.layers.replication import BASE_TO_REPLICA_MAP, REPLICATED_PROPERTIES_TABLE\n", "\n", "logging.basicConfig(\n", "    stream=sys.stdout,\n", "    format='%(asctime)s - %(levelname)s - %(message)s',\n", "    level=logging.INFO\n", ")\n", "log = logging.getLogger(__name__)\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class DataData_Data_1(DataSupervised):\n", "    \"\"\"Class responsible for loading data from files (e.g., numpy, csv, etc).\"\"\"    \n", "    def __init__(self):\n", "        self._variables = {}\n", "        self._columns = []\n", "\n", "        columns = {}\n", "        trn_sz_tot, val_sz_tot, tst_sz_tot = 0, 0, 0        \n", "        trn_gens_args_DataData_Data_1, val_gens_args_DataData_Data_1, tst_gens_args_DataData_Data_1 = [], [], []        \n", "\n", "        columns_DataData_Data_1_0 = None\n", "\n", "        global matrix_DataData_Data_1_0\n", "        matrix_DataData_Data_1_0 = np.load(\"C:/Users/Robert/Documents/PerceptiLabs/PereptiLabsPlatform/Data/mnist_split/mnist_input.npy\", mmap_mode='r+').astype(np.float32)\n", "        size_DataData_Data_1_0 = len(matrix_DataData_Data_1_0)\n", "\n", "        def generator_DataData_Data_1_0(idx_lo, idx_hi):\n", "            global matrix_DataData_Data_1_0\n", "            yield from matrix_DataData_Data_1_0[idx_lo:idx_hi].squeeze()\n", "\n", "\n", "        if columns_DataData_Data_1_0 is not None:\n", "            columns[\"DataData_Data_1_0\"] = columns_DataData_Data_1_0\n", "            self._columns = columns_DataData_Data_1_0\n", "\n", "        trn_sz = int(round(0.01*70*size_DataData_Data_1_0))\n", "        val_sz = int(round(0.01*20*size_DataData_Data_1_0))\n", "        tst_sz = int(size_DataData_Data_1_0 - trn_sz - val_sz)\n", "\n", "        trn_sz_tot += trn_sz\n", "        val_sz_tot += val_sz\n", "        tst_sz_tot += tst_sz\n", "        \n", "        trn_gens_args_DataData_Data_1.append((generator_DataData_Data_1_0, 0, trn_sz))\n", "        val_gens_args_DataData_Data_1.append((generator_DataData_Data_1_0, trn_sz, trn_sz+val_sz))\n", "        tst_gens_args_DataData_Data_1.append((generator_DataData_Data_1_0, trn_sz+val_sz, trn_sz+val_sz+tst_sz))\n", "                    \n", "        self._trn_gens_args = trn_gens_args_DataData_Data_1\n", "        self._val_gens_args = val_gens_args_DataData_Data_1                                        \n", "        self._tst_gens_args = tst_gens_args_DataData_Data_1\n", "                    \n", "        self._trn_sz_tot = trn_sz_tot\n", "        self._val_sz_tot = val_sz_tot\n", "        self._tst_sz_tot = tst_sz_tot\n", "\n", "        self._variables = {k: v for k, v in locals().items() if can_serialize(v)}\n", "\n", "    @property\n", "    def variables(self) -> Dict[str, Picklable]:\n", "        \"\"\"Returns any variables that the layer should make available and that can be pickled.\"\"\"\n", "        return self._variables\n", "\n", "    @property\n", "    def sample(self) -> np.ndarray:\n", "        \"\"\"Returns a single data sample\"\"\"                    \n", "        sample = next(self.make_generator_training())\n", "        return sample\n", "\n", "    @property\n", "    def columns(self) -> List[str]:\n", "        \"\"\"Column names. Corresponds to each column in a sample \"\"\"\n", "        return self._columns.copy()\n", "\n", "    @property\n", "    def size_training(self) -> int:\n", "        \"\"\"Returns the size of the training dataset\"\"\"                    \n", "        return self._trn_sz_tot\n", "\n", "    @property\n", "    def size_validation(self) -> int:\n", "        \"\"\"Returns the size of the validation dataset\"\"\"\n", "        return self._val_sz_tot\n", "\n", "    @property\n", "    def size_testing(self) -> int:\n", "        \"\"\"Returns the size of the testing dataset\"\"\"                    \n", "        return self._tst_sz_tot\n", "                    \n", "    def make_generator_training(self) -> Generator[np.ndarray, None, None]:\n", "        \"\"\"Returns a generator yielding single samples of training data.\"\"\"                                        \n", "        def gen():\n", "            for fn, lo, hi in self._trn_gens_args:\n", "                for x in fn(lo, hi):\n", "                    self._output = x\n", "                    yield x\n", "        return gen()\n", "        \n", "    def make_generator_validation(self) -> Generator[np.ndarray, None, None]:\n", "        \"\"\"Returns a generator yielding single samples of validation data.\"\"\"                    \n", "        def gen():\n", "            for fn, lo, hi in self._val_gens_args:\n", "                for x in fn(lo, hi):\n", "                    self._output = x\n", "                    yield x\n", "        return gen()\n", "\n", "    def make_generator_testing(self) -> Generator[np.ndarray, None, None]:\n", "        \"\"\"Returns a generator yielding single samples of testing data.\"\"\"                            \n", "        def gen():\n", "            for fn, lo, hi in self._tst_gens_args:\n", "                for x in fn(lo, hi):\n", "                    self._output = x\n", "                    yield x\n", "        return gen()\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class DataData_Data_2(DataSupervised):\n", "    \"\"\"Class responsible for loading data from files (e.g., numpy, csv, etc).\"\"\"    \n", "    def __init__(self):\n", "        self._variables = {}\n", "        self._columns = []\n", "\n", "        columns = {}\n", "        trn_sz_tot, val_sz_tot, tst_sz_tot = 0, 0, 0        \n", "        trn_gens_args_DataData_Data_2, val_gens_args_DataData_Data_2, tst_gens_args_DataData_Data_2 = [], [], []        \n", "\n", "        columns_DataData_Data_2_0 = None\n", "\n", "        global matrix_DataData_Data_2_0\n", "        matrix_DataData_Data_2_0 = np.load(\"C:/Users/Robert/Documents/PerceptiLabs/PereptiLabsPlatform/Data/mnist_split/mnist_labels.npy\", mmap_mode='r+').astype(np.float32)\n", "        size_DataData_Data_2_0 = len(matrix_DataData_Data_2_0)\n", "\n", "        def generator_DataData_Data_2_0(idx_lo, idx_hi):\n", "            global matrix_DataData_Data_2_0\n", "            yield from matrix_DataData_Data_2_0[idx_lo:idx_hi].squeeze()\n", "\n", "\n", "        if columns_DataData_Data_2_0 is not None:\n", "            columns[\"DataData_Data_2_0\"] = columns_DataData_Data_2_0\n", "            self._columns = columns_DataData_Data_2_0\n", "\n", "        trn_sz = int(round(0.01*70*size_DataData_Data_2_0))\n", "        val_sz = int(round(0.01*20*size_DataData_Data_2_0))\n", "        tst_sz = int(size_DataData_Data_2_0 - trn_sz - val_sz)\n", "\n", "        trn_sz_tot += trn_sz\n", "        val_sz_tot += val_sz\n", "        tst_sz_tot += tst_sz\n", "        \n", "        trn_gens_args_DataData_Data_2.append((generator_DataData_Data_2_0, 0, trn_sz))\n", "        val_gens_args_DataData_Data_2.append((generator_DataData_Data_2_0, trn_sz, trn_sz+val_sz))\n", "        tst_gens_args_DataData_Data_2.append((generator_DataData_Data_2_0, trn_sz+val_sz, trn_sz+val_sz+tst_sz))\n", "                    \n", "        self._trn_gens_args = trn_gens_args_DataData_Data_2\n", "        self._val_gens_args = val_gens_args_DataData_Data_2                                        \n", "        self._tst_gens_args = tst_gens_args_DataData_Data_2\n", "                    \n", "        self._trn_sz_tot = trn_sz_tot\n", "        self._val_sz_tot = val_sz_tot\n", "        self._tst_sz_tot = tst_sz_tot\n", "\n", "        self._variables = {k: v for k, v in locals().items() if can_serialize(v)}\n", "\n", "    @property\n", "    def variables(self) -> Dict[str, Picklable]:\n", "        \"\"\"Returns any variables that the layer should make available and that can be pickled.\"\"\"\n", "        return self._variables\n", "\n", "    @property\n", "    def sample(self) -> np.ndarray:\n", "        \"\"\"Returns a single data sample\"\"\"                    \n", "        sample = next(self.make_generator_training())\n", "        return sample\n", "\n", "    @property\n", "    def columns(self) -> List[str]:\n", "        \"\"\"Column names. Corresponds to each column in a sample \"\"\"\n", "        return self._columns.copy()\n", "\n", "    @property\n", "    def size_training(self) -> int:\n", "        \"\"\"Returns the size of the training dataset\"\"\"                    \n", "        return self._trn_sz_tot\n", "\n", "    @property\n", "    def size_validation(self) -> int:\n", "        \"\"\"Returns the size of the validation dataset\"\"\"\n", "        return self._val_sz_tot\n", "\n", "    @property\n", "    def size_testing(self) -> int:\n", "        \"\"\"Returns the size of the testing dataset\"\"\"                    \n", "        return self._tst_sz_tot\n", "                    \n", "    def make_generator_training(self) -> Generator[np.ndarray, None, None]:\n", "        \"\"\"Returns a generator yielding single samples of training data.\"\"\"                                        \n", "        def gen():\n", "            for fn, lo, hi in self._trn_gens_args:\n", "                for x in fn(lo, hi):\n", "                    self._output = x\n", "                    yield x\n", "        return gen()\n", "        \n", "    def make_generator_validation(self) -> Generator[np.ndarray, None, None]:\n", "        \"\"\"Returns a generator yielding single samples of validation data.\"\"\"                    \n", "        def gen():\n", "            for fn, lo, hi in self._val_gens_args:\n", "                for x in fn(lo, hi):\n", "                    self._output = x\n", "                    yield x\n", "        return gen()\n", "\n", "    def make_generator_testing(self) -> Generator[np.ndarray, None, None]:\n", "        \"\"\"Returns a generator yielding single samples of testing data.\"\"\"                            \n", "        def gen():\n", "            for fn, lo, hi in self._tst_gens_args:\n", "                for x in fn(lo, hi):\n", "                    self._output = x\n", "                    yield x\n", "        return gen()\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class ProcessReshape_Reshape_1(Tf1xLayer):\n", "    def __call__(self, x: tf.Tensor, is_training: tf.Tensor = None) -> tf.Tensor:\n", "        \"\"\" Takes a tensor as input and reshapes it.\"\"\"\n", "        shp = [28, 28, 1]\n", "        perm = [0, 1, 2]\n", "        \n", "        shp = [i for i in shp if i != 0]\n", "        if(len(shp) != len(perm)):\n", "            perm = []\n", "            for i in range(len(shp)):\n", "                perm.append(i)\n", "            \n", "        y = tf.reshape(x, [x.get_shape().as_list()[0] if x.get_shape().as_list()[0] is not None else -1] + shp)\n", "        y = tf.transpose(y, perm=[0] + [i+1 for i in perm])\n", "        \n", "        self.y = y\n", "        return y\n", "\n", "    @property\n", "    def variables(self) -> Dict[str, Picklable]:\n", "        \"\"\"Any variables belonging to this layer that should be rendered in the frontend.\n", "        \n", "        Returns:\n", "            A dictionary with tensor names for keys and picklable for values.\n", "        \"\"\"\n", "        return {}\n", "\n", "    @property\n", "    def trainable_variables(self) -> Dict[str, tf.Tensor]:\n", "        \"\"\"Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.\n", "        \n", "        Returns:\n", "            A dictionary with tensor names for keys and tensors for values.\n", "        \"\"\"\n", "        return {}\n", "    \n", "    def get_sample(self, sess=None) -> np.ndarray:\n", "        \"\"\"Returns a single data sample\"\"\"\n", "        if sess is not None:\n", "            y = sess.run(self.y)\n", "            return y[0]\n", "        else:\n", "            return None\n", "\n", "    @property\n", "    def weights(self) -> Dict[str, tf.Tensor]:\n", "        \"\"\"Any weight tensors belonging to this layer that should be rendered in the frontend.\n", "\n", "        Return:\n", "            A dictionary with tensor names for keys and tensors for values.\n", "        \"\"\"        \n", "        return {}\n", "\n", "    @property\n", "    def biases(self) -> Dict[str, tf.Tensor]:\n", "        \"\"\"Any weight tensors belonging to this layer that should be rendered in the frontend.\n", "\n", "        Return:\n", "            A dictionary with tensor names for keys and tensors for values.\n", "        \"\"\"        \n", "        return {}        \n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class ProcessOneHot_OneHot_1(Tf1xLayer):\n", "    def __call__(self, x, is_training: tf.Tensor = None):\n", "        y = tf.one_hot(tf.cast(x, dtype=tf.int32), 10)       \n", "        self.y = y\n", "\n", "        return y\n", "\n", "    @property\n", "    def variables(self) -> Dict[str, Picklable]:\n", "        \"\"\"Any variables belonging to this layer that should be rendered in the frontend.\n", "        \n", "        Returns:\n", "            A dictionary with tensor names for keys and picklable for values.\n", "        \"\"\"\n", "        return {}\n", "\n", "    @property\n", "    def trainable_variables(self) -> Dict[str, tf.Tensor]:\n", "        \"\"\"Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.\n", "        \n", "        Returns:\n", "            A dictionary with tensor names for keys and tensors for values.\n", "        \"\"\"\n", "        return {}\n", "    \n", "    def get_sample(self, sess=None) -> np.ndarray:\n", "        \"\"\"Returns a single data sample\"\"\"\n", "        if sess is not None:\n", "            y = sess.run(self.y)\n", "            return y[0]\n", "        else:\n", "            return None\n", "\n", "    @property\n", "    def weights(self) -> Dict[str, tf.Tensor]:\n", "        \"\"\"Any weight tensors belonging to this layer that should be rendered in the frontend.\n", "\n", "        Return:\n", "            A dictionary with tensor names for keys and tensors for values.\n", "        \"\"\"        \n", "        return {}\n", "\n", "    @property    \n", "    def biases(self) -> Dict[str, tf.Tensor]:\n", "        \"\"\"Any weight tensors belonging to this layer that should be rendered in the frontend.\n", "\n", "        Return:\n", "            A dictionary with tensor names for keys and tensors for values.\n", "        \"\"\"        \n", "        return {}    \n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class DeepLearningConv_Convolution_1(Tf1xLayer):\n", "    def __init__(self):\n", "        self._scope = 'DeepLearningConv_Convolution_1'        \n", "        # TODO: implement support for 1d and 3d conv, dropout, funcs, pooling, etc\n", "        self._patch_size = 3\n", "        self._feature_maps = 8\n", "        self._padding = 'SAME'\n", "        self._stride = 2\n", "        self._keep_prob = 1\n", "        self._variables = {}\n", "        self._lw_variables = {}\n", "        \n", "    def __call__(self, x, is_training: tf.Tensor = None):\n", "        \"\"\" Takes a tensor as input and feeds it forward through a convolutional layer, returning a newtensor.\"\"\"\n", "        b_norm = False\n", "        y_before = None\n", "        y = None\n", "\n", "        with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):\n", "            shape = [\n", "                self._patch_size,\n", "                self._patch_size,\n", "                x.get_shape().as_list()[-1],\n", "                self._feature_maps\n", "            ]\n", "            #initial = tf.random.truncated_normal(\n", "            #    shape,\n", "            #   stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)\n", "            #)\n", "            W = tf.compat.v1.get_variable('W', shape = shape, initializer=  tf.contrib.layers.xavier_initializer())\n", "            \n", "            #initial = tf.constant(0.1, shape=[self._feature_maps])\n", "            b = tf.compat.v1.get_variable('b', shape=[self._feature_maps], initializer=tf.zeros_initializer())\n", "            y = tf.add(tf.nn.conv2d(x, W, strides=[1, self._stride, self._stride, 1], padding=self._padding), b)\n", "\n", "            \n", "\n", "            y = tf.compat.v1.sigmoid(y)\n", "        \n", "        self._variables = {k: v for k, v in locals().items() if can_serialize(v)}\n", "        self._lw_variables['y_before'] = y_before\n", "        self._lw_variables['y'] = y\n", "\n", "        return y\n", "\n", "    @property\n", "    def variables(self):\n", "        \"\"\"Any variables belonging to this layer that should be rendered in the frontend.\n", "        \n", "        Returns:\n", "            A dictionary with tensor names for keys and picklable for values.\n", "        \"\"\"\n", "        return self._variables.copy()\n", "\n", "    @property\n", "    def trainable_variables(self):\n", "        \"\"\"Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.\n", "        \n", "        Returns:\n", "            A dictionary with tensor names for keys and tensors for values.\n", "        \"\"\"\n", "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)\n", "        variables = {v.name: v for v in variables}\n", "        return variables        \n", "\n", "    def get_sample(self, sess=None) -> np.ndarray:\n", "        \"\"\"Returns a single data sample\"\"\"\n", "        if sess is not None:\n", "            y_before = self._lw_variables.get('y_before')\n", "\n", "            if y_before is not None:\n", "                y = sess.run(y_before)\n", "            else:\n", "                y = sess.run(self._lw_variables['y'])\n", "            \n", "            return y[0]\n", "        else:\n", "            return None\n", "\n", "    @property\n", "    def weights(self):\n", "        \"\"\"Any weight tensors belonging to this layer that should be rendered in the frontend.\n", "\n", "        Return:\n", "            A dictionary with tensor names for keys and tensors for values.\n", "        \"\"\"        \n", "        with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):\n", "            w = tf.compat.v1.get_variable('W')\n", "            return {w.name: w}\n", "\n", "    @property\n", "    def biases(self):\n", "        \"\"\"Any weight tensors belonging to this layer that should be rendered in the frontend.\n", "\n", "        Return:\n", "            A dictionary with tensor names for keys and tensors for values.\n", "        \"\"\"        \n", "        with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):\n", "            b = tf.compat.v1.get_variable('b')\n", "            return {b.name: b}\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class DeepLearningFC_Fully_Connected_1(Tf1xLayer):\n", "    def __init__(self):\n", "        self._scope = 'DeepLearningFC_Fully_Connected_1'\n", "        self._n_neurons = 10\n", "        self._variables = {}\n", "        self._lw_variables = {}\n", "        \n", "    def __call__(self, x: tf.Tensor, is_training: tf.Tensor = None):\n", "        \"\"\" Takes a tensor as input and feeds it forward through a layer of neurons, returning a newtensor.\"\"\"        \n", "        n_neurons = 10\n", "        n_inputs = np.prod(x.get_shape().as_list()[1:], dtype=np.int32)\n", "        b_norm = False\n", "        y_before = None\n", "        y = None\n", "\n", "        with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):        \n", "            initial = tf.random.truncated_normal((n_inputs, self._n_neurons), stddev=0.1)\n", "            W = tf.compat.v1.get_variable('W', initializer=initial)\n", "\n", "            initial = tf.constant(0.1, shape=[self._n_neurons])\n", "            b = tf.compat.v1.get_variable('b', initializer=initial)\n", "            flat_node = tf.cast(tf.reshape(x, [-1, n_inputs]), dtype=tf.float32)\n", "            y = tf.matmul(flat_node, W) + b\n", "            \n", "            \n", "\n", "            y = tf.compat.v1.sigmoid(y)\n", "\n", "            \n", "        self._variables = {k: v for k, v in locals().items() if can_serialize(v)}\n", "        self._lw_variables['y_before'] = y_before\n", "        self._lw_variables['y'] = y\n", "\n", "        return y\n", "\n", "    @property\n", "    def variables(self):\n", "        \"\"\"Any variables belonging to this layer that should be rendered in the frontend.\n", "        \n", "        Returns:\n", "            A dictionary with tensor names for keys and picklable for values.\n", "        \"\"\"\n", "        return self._variables.copy()\n", "\n", "    @property\n", "    def trainable_variables(self):\n", "        \"\"\"Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.\n", "        \n", "        Returns:\n", "            A dictionary with tensor names for keys and tensors for values.\n", "        \"\"\"\n", "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)\n", "        variables = {v.name: v for v in variables}\n", "        return variables\n", "\n", "    def get_sample(self, sess=None) -> np.ndarray:\n", "        \"\"\"Returns a single data sample\"\"\"\n", "        if sess is not None:\n", "            y_before = self._lw_variables.get('y_before')\n", "\n", "            if y_before is not None:\n", "                y = sess.run(y_before)\n", "            else:\n", "                y = sess.run(self._lw_variables['y'])\n", "            \n", "            return y[0]\n", "        else:\n", "            return None\n", "\n", "    @property\n", "    def weights(self):\n", "        \"\"\"Any weight tensors belonging to this layer that should be rendered in the frontend.\n", "\n", "        Return:\n", "            A dictionary with tensor names for keys and tensors for values.\n", "        \"\"\"        \n", "        with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):\n", "            w = tf.compat.v1.get_variable('W')\n", "            return {w.name: w}\n", "\n", "    @property\n", "    def biases(self):\n", "        \"\"\"Any weight tensors belonging to this layer that should be rendered in the frontend.\n", "\n", "        Return:\n", "            A dictionary with tensor names for keys and tensors for values.\n", "        \"\"\"        \n", "        with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):\n", "            b = tf.compat.v1.get_variable('b')\n", "            return {b.name: b}    \n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class TrainNormal_Normal_1(ClassificationLayer):\n", "\n", "    def __init__(self):\n", "        self._n_epochs = 10\n", "        self._batch_size = 10\n", "\n", "        self._stopped = False\n", "        self._paused = False\n", "        self._headless = False\n", "        self._status = 'created'\n", "        \n", "        self._loss_training = 0.0\n", "        self._loss_validation = 0.0\n", "        self._loss_testing = 0.0      \n", "\n", "        self._accuracy_training = 0.0\n", "        self._accuracy_validation = 0.0\n", "        self._accuracy_testing = 0.0      \n", "        \n", "        self._variables = {}\n", "        self._layer_outputs = {}\n", "        self._layer_weights = {}\n", "        self._layer_biases = {}        \n", "        self._layer_gradients = {}\n", "\n", "        self._training_iteration = 0\n", "        self._validation_iteration = 0\n", "        self._testing_iteration = 0\n", "\n", "        self._trn_sz_tot = 0\n", "        self._val_sz_tot = 0\n", "        self._tst_sz_tot = 0\n", "\n", "        self._checkpoint = None\n", "        \n", "    def run(self, graph: Graph):\n", "        \"\"\"Called as the main entry point for training. Responsible for training the model.\n", "\n", "        Args:\n", "            graph: A PerceptiLabs Graph object containing references to all layers objects included in the model produced by this training layer.\n", "        \"\"\"   \n", "        self._status = 'initializing'\n", "        output_layer_id = '_Fully_Connected_1'\n", "        target_layer_id = '_OneHot_1'\n", "        input_data_nodes = graph.get_direct_data_nodes(output_layer_id)\n", "        label_data_nodes = graph.get_direct_data_nodes(target_layer_id)\n", "\n", "        assert len(input_data_nodes) == 1\n", "        assert len(label_data_nodes) == 1\n", "        input_data_node = input_data_nodes[0]\n", "        label_data_node = label_data_nodes[0]\n", "        \n", "        self._trn_sz_tot = input_data_node.layer.size_training\n", "        self._val_sz_tot = input_data_node.layer.size_validation\n", "        self._tst_sz_tot = input_data_node.layer.size_testing\n", "\n", "        # Make training set\n", "        dataset_trn = tf.data.Dataset.zip((\n", "            tf.data.Dataset.from_generator(\n", "                input_data_node.layer_instance.make_generator_training,\n", "                output_shapes=input_data_node.layer_instance.sample.shape,\n", "                output_types=np.float32                \n", "            ),\n", "            tf.data.Dataset.from_generator(\n", "                label_data_node.layer_instance.make_generator_training,\n", "                output_shapes=label_data_node.layer_instance.sample.shape,\n", "                output_types=np.float32\n", "            )\n", "        ))\n", "\n", "        # Make validation set\n", "        dataset_val = tf.data.Dataset.zip((\n", "            tf.data.Dataset.from_generator(\n", "                input_data_node.layer_instance.make_generator_validation,\n", "                output_shapes=input_data_node.layer_instance.sample.shape,\n", "                output_types=np.float32                \n", "            ),\n", "            tf.data.Dataset.from_generator(\n", "                label_data_node.layer_instance.make_generator_validation,\n", "                output_shapes=label_data_node.layer_instance.sample.shape,\n", "                output_types=np.float32\n", "            )\n", "        ))\n", "\n", "        # Make testing set\n", "        dataset_tst = tf.data.Dataset.zip((\n", "            tf.data.Dataset.from_generator(\n", "                input_data_node.layer_instance.make_generator_testing,\n", "                output_shapes=input_data_node.layer_instance.sample.shape,\n", "                output_types=np.float32                \n", "            ),\n", "            tf.data.Dataset.from_generator(\n", "                label_data_node.layer_instance.make_generator_testing,\n", "                output_shapes=label_data_node.layer_instance.sample.shape,\n", "                output_types=np.float32\n", "            )\n", "        ))\n", "        \n", "        dataset_trn = dataset_trn.batch(self._batch_size)\n", "        dataset_val = dataset_val.batch(self._batch_size)\n", "        dataset_tst = dataset_tst.batch(1)                \n", "\n", "        # Make initializers\n", "        with tf.variable_scope('TrainNormal_Normal_1/train', reuse=tf.AUTO_REUSE):\n", "            is_training = tf.get_variable(name=\"is_train\", dtype=tf.bool, initializer=False)\n", "\n", "        iterator = tf.data.Iterator.from_structure(dataset_trn.output_types, dataset_trn.output_shapes)\n", "\n", "        trn_init = iterator.make_initializer(dataset_trn)\n", "        trn_init = tf.group([trn_init, is_training.assign(True if self._batch_size > 1 else False)])\n", "        \n", "        val_init = iterator.make_initializer(dataset_val)\n", "        val_init = tf.group([val_init, is_training.assign(False)])\n", "\n", "        tst_init = iterator.make_initializer(dataset_tst)        \n", "        tst_init = tf.group([tst_init, is_training.assign(False)])\n", "\n", "        input_tensor, label_tensor = iterator.get_next()\n", "\n", "        # Build the TensorFlow graph # TODO: perhaps this part can be delegated to the graph?\n", "\n", "        def build_graph(input_tensor, label_tensor):\n", "            layer_output_tensors = {\n", "                input_data_node.layer_id: input_tensor,\n", "                label_data_node.layer_id: label_tensor\n", "            }\n", "\n", "            for node in graph.inner_nodes:\n", "                args = []\n", "                for input_node in graph.get_input_nodes(node):\n", "                    args.append(layer_output_tensors[input_node.layer_id])\n", "                    args.append(is_training)\n", "\n", "                    y = node.layer_instance(*args)\n", "                layer_output_tensors[node.layer_id] = y\n", "\n", "\n", "            return layer_output_tensors\n", "\n", "        layer_output_tensors = build_graph(input_tensor, label_tensor)\n", "        output_tensor = layer_output_tensors[output_layer_id]\n", "        target_tensor = layer_output_tensors[target_layer_id]\n", "        \n", "        update_ops = tf.compat.v1.get_collection(tf.GraphKeys.UPDATE_OPS)\n", "        \n", "        # Create an exportable version of the TensorFlow graph\n", "        self._input_tensor_export = tf.placeholder(shape=dataset_trn.output_shapes[0], dtype=dataset_trn.output_types[0])\n", "        self._output_tensor_export = build_graph(\n", "            self._input_tensor_export,\n", "            tf.placeholder(shape=dataset_trn.output_shapes[1], dtype=dataset_trn.output_types[1])\n", "        )[output_layer_id]\n", "        \n", "        # Defining loss function\n", "        loss_tensor = tf.reduce_mean(tf.square(output_tensor - target_tensor))\n", "\n", "        correct_predictions = tf.equal(tf.argmax(output_tensor, -1), tf.argmax(target_tensor, -1))\n", "        accuracy_tensor = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n", "\n", "        global_step = None\n", "\n", "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999)\n", "\n", "        layer_weight_tensors = {}\n", "        layer_bias_tensors = {}        \n", "        layer_gradient_tensors = {}\n", "        for node in graph.inner_nodes:\n", "            if not isinstance(node.layer, Tf1xLayer): # In case of pure custom layers...\n", "                continue\n", "            \n", "            layer_weight_tensors[node.layer_id] = node.layer.weights\n", "            layer_bias_tensors[node.layer_id] = node.layer.biases            \n", "            \n", "            if len(node.layer.trainable_variables) > 0:\n", "                gradients = {}\n", "                for name, tensor in node.layer.trainable_variables.items():\n", "                    grad_tensor = tf.gradients(loss_tensor, tensor)\n", "                    if any(x is None for x in grad_tensor):\n", "                        grad_tensor = tf.constant(0)\n", "                    gradients[name] = grad_tensor\n", "                layer_gradient_tensors[node.layer_id] = gradients\n", "                # self._internal_layer_gradients[node.layer_id] = {name: [] for name in node.layer.trainable_variables.keys()} # Initialize\n", "                # self._layer_gradients = self._internal_layer_gradients.copy()\n", "\n", "        # trainable_vars = tf.trainable_variables() # TODO: safer to get from nodes. Especially with split graph in mind.\n", "        # grads = tf.gradients(loss_tensor, trainable_vars)\n", "        # update_weights = optimizer.apply_gradients(zip(grads, trainable_vars), global_step=global_step)        \n", "        \n", "        update_weights = optimizer.minimize(loss_tensor, global_step=global_step)\n", "        update_weights = tf.group([update_weights, update_ops])\n", "\n", "        config = tf.ConfigProto()\n", "        config.gpu_options.allow_growth = True\n", "        sess = tf.Session(config=config)\n", "        self._sess = sess\n", "\n", "        trackable_variables = {}\n", "        trackable_variables.update({x.name: x for x in tf.trainable_variables() if isinstance(x, Trackable)})\n", "        trackable_variables.update({k: v for k, v in locals().items() if isinstance(v, Trackable) and not isinstance(v, tf.python.data.ops.iterator_ops.Iterator)}) # TODO: Iterators based on 'stateful functions' cannot be serialized.\n", "        self._checkpoint = tf.train.Checkpoint(**trackable_variables)\n", "\n", "        sess.run(tf.global_variables_initializer())\n", "        \n", "                    \n", "\n", "        def train_step():\n", "            if not self._headless:\n", "                _, self._loss_training, self._accuracy_training, \\\n", "                    self._layer_outputs, self._layer_weights, self._layer_biases, \\\n", "                    self._layer_gradients \\\n", "                    = sess.run([\n", "                        update_weights, loss_tensor, accuracy_tensor,\n", "                        layer_output_tensors, layer_weight_tensors, layer_bias_tensors, layer_gradient_tensors\n", "                    ])\n", "            else:\n", "                _, self._loss_training, self._accuracy_training, \\\n", "                    = sess.run([\n", "                        update_weights, loss_tensor, accuracy_tensor\n", "                    ])\n", "            \n", "        def validation_step():\n", "            if not self._headless:\n", "                self._loss_validation, self._accuracy_validation, \\\n", "                    self._layer_outputs, self._layer_weights, self._layer_biases, \\\n", "                    self._layer_gradients \\\n", "                    = sess.run([\n", "                        loss_tensor, accuracy_tensor,\n", "                        layer_output_tensors, layer_weight_tensors, layer_bias_tensors, layer_gradient_tensors\n", "                    ])\n", "            else:\n", "                self._loss_validation, self._accuracy_validation, \\\n", "                    = sess.run([\n", "                        loss_tensor, accuracy_tensor\n", "                    ])\n", "\n", "            \n", "        def test_step():\n", "            self._loss_testing, self._accuracy_testing, \\\n", "                self._layer_outputs, self._layer_weights, self._layer_gradients \\\n", "                = sess.run([\n", "                    loss_tensor, accuracy_tensor,\n", "                    layer_output_tensors, layer_weight_tensors, layer_gradient_tensors\n", "                ])\n", "            #accuracy_list.append(acc)\n", "            #loss_list.append(loss)\n", "\n", "        self._variables = {k: v for k, v in locals().items() if can_serialize(v)}\n", "\n", "        log.info(\"Entering training loop\")\n", "\n", "        # Training loop\n", "        self._epoch = 0\n", "        while self._epoch < self._n_epochs and not self._stopped:\n", "            t0 = time.perf_counter()\n", "            self._training_iteration = 0\n", "            self._validation_iteration = 0\n", "            self._status = 'training'\n", "            sess.run(trn_init)            \n", "            try:\n", "                while not self._stopped:\n", "                    train_step()\n", "                    yield YieldLevel.SNAPSHOT\n", "                    self._training_iteration += 1\n", "            except tf.errors.OutOfRangeError:\n", "                pass\n", "\n", "            self._status = 'validation'\n", "            sess.run(val_init)            \n", "            try:\n", "                while not self._stopped:\n", "                    validation_step()\n", "                    yield YieldLevel.SNAPSHOT                    \n", "                    self._validation_iteration += 1\n", "            except tf.errors.OutOfRangeError:\n", "                pass\n", "            log.info(\n", "                f\"Finished epoch {self._epoch+1}/{self._n_epochs} - \"\n", "                f\"loss training, validation: {self.loss_training:.6f}, {self.loss_validation:.6f} - \"\n", "                f\"acc. training, validation: {self.accuracy_training:.6f}, {self.accuracy_validation:.6f}\"\n", "            )\n", "            log.info(f\"Epoch duration: {round(time.perf_counter() - t0, 3)} s\")            \n", "            self._epoch += 1\n", "\n", "        self._variables = {k: v for k, v in locals().items() if can_serialize(v)}            \n", "        yield YieldLevel.DEFAULT            \n", "        \n", "        # Test loop\n", "        self._testing_iteration = 0\n", "        self._status = 'testing'\n", "        sess.run(tst_init)                                \n", "        try:\n", "            while not self._stopped:\n", "                test_step()\n", "                yield YieldLevel.SNAPSHOT\n", "                self._testing_iteration += 1\n", "        except tf.errors.OutOfRangeError:\n", "            pass\n", "\n", "        self._status = 'finished'\n", "        self._variables = {k: v for k, v in locals().items() if can_serialize(v)}\n", "        yield YieldLevel.DEFAULT\n", "\n", "                \n", "\n", "    def on_export(self, path: str, mode: str) -> None:\n", "        \"\"\"Called when the export or save button is clicked in the frontend.\n", "        It is up to the implementing layer to save the model to disk.\n", "        \n", "        Args:\n", "            path: the directory where the exported model will be stored.\n", "            mode: how to export the model. Made available to frontend via 'export_modes' property.\"\"\"\n", "\n", "        log.debug(f\"Export called. Project path = {path}, mode = {mode}\")\n", "        pb_path = os.path.join(path, '1')\n", "\n", "        if os.path.exists(pb_path):\n", "            shutil.rmtree(pb_path)\n", "\n", "        os.makedirs(pb_path, exist_ok=True)\n", "        \n", "        import pdb; pdb.set_trace()\n", "\n", "        # Export non-compressed model\n", "        if mode in ['TFModel', 'TFModel+checkpoint']:\n", "            tf.compat.v1.saved_model.simple_save(self._sess, pb_path, inputs={'input': self._input_tensor_export}, outputs={'output': self._output_tensor_export})\n", "\n", "        # Export compressed model\n", "        if mode in ['TFLite', 'TFLite+checkpoint']:\n", "            frozen_path = os.path.join(pb_path, 'frozen_model.pb')\n", "            converter = tf.lite.TFLiteConverter.from_session(self._sess, [self._input_tensor_export], [self._output_tensor_export])\n", "            converter.post_training_quantize = True\n", "            tflite_model = converter.convert()\n", "            with open(frozen_path, \"wb\") as f:\n", "                f.write(tflite_model)\n", "\n", "        # Export checkpoint\n", "        if mode in ['TFModel+checkpoint', 'TFLite+checkpoint']:\n", "            self._checkpoint.save(file_prefix=os.path.join(path, 'model.ckpt'), session=self._sess)\n", "                \n", "    def on_stop(self) -> None:\n", "        \"\"\"Called when the save model button is clicked in the frontend. \n", "        It is up to the implementing layer to save the model to disk.\"\"\"\n", "        self._stopped = True\n", "\n", "    def on_headless_activate(self) -> None:\n", "        \"\"\"\"Called when the statistics shown in statistics window are not needed.\n", "        Purose is to speed up the iteration speed significantly.\"\"\"\n", "        self._headless = True\n", "\n", "        self._layer_outputs = {} \n", "        self._layer_weights = {}\n", "        self._layer_biases = {}\n", "        self._layer_gradients = {}\n", "\n", "    def on_headless_deactivate(self) -> None:\n", "        \"\"\"\"Called when the statistics shown in statistics window are needed.\n", "        May slow down the iteration speed of the training.\"\"\"\n", "        import time\n", "        log.info(f\"Set to headless_off at time {time.time()}\")\n", "        self._headless = False\n", "\n", "    @property\n", "    def export_modes(self) -> List[str]:\n", "        \"\"\"Returns the possible modes of exporting a model.\"\"\"        \n", "        return [\n", "            'TFModel',\n", "            'TFLite'\n", "            'TFModel+checkpoint',\n", "            'TFLite+checkpoint',            \n", "        ]\n", "        \n", "    @property\n", "    def is_paused(self) -> None:\n", "        \"\"\"Returns true when the training is paused.\"\"\"        \n", "        return self._paused\n", "\n", "    @property\n", "    def batch_size(self):\n", "        \"\"\" Size of the current training batch \"\"\"        \n", "        return self._batch_size\n", "\n", "    @property\n", "    def status(self):\n", "        \"\"\"Called when the pause button is clicked in the frontend. It is up to the implementing layer to pause its execution.\"\"\"        \n", "        return self._status\n", "    \n", "    @property\n", "    def epoch(self):\n", "        \"\"\"The current epoch\"\"\"        \n", "        return self._epoch\n", "\n", "    @property\n", "    def variables(self):\n", "        \"\"\"Any variables belonging to this layer that should be rendered in the frontend.\n", "        \n", "        Returns:\n", "            A dictionary with tensor names for keys and picklable for values.\n", "        \"\"\"\n", "        return self._variables.copy()        \n", "\n", "    @property\n", "    def sample(self) -> np.ndarray:\n", "        \"\"\"Returns a single data sample\"\"\"        \n", "        return np.empty(())\n", "\n", "    @property\n", "    def columns(self) -> List[str]: \n", "        \"\"\"Column names. Corresponds to each column in a sample \"\"\"\n", "        return []\n", "\n", "    @property\n", "    def size_training(self) -> int:\n", "        \"\"\"Returns the size of the training dataset\"\"\"                                    \n", "        return self._trn_sz_tot\n", "\n", "    @property\n", "    def size_validation(self) -> int:\n", "        \"\"\"Returns the size of the validation dataset\"\"\"                                            \n", "        return self._val_sz_tot\n", "\n", "    @property\n", "    def size_testing(self) -> int:\n", "        \"\"\"Returns the size of the testing dataset\"\"\"\n", "        return self._tst_sz_tot\n", "\n", "    def make_generator_training(self) -> Generator[np.ndarray, None, None]:\n", "        \"\"\"Returns a generator yielding single samples of training data. In the case of a training layer, this typically yields the model output.\"\"\"        \n", "        # Simply call sess.run on the output & target tensors :)  #TODO: how to make generators generic? We have two datasets here, but not all datasets will be labeled. Distinguish between supervised/unsupervised data layers and instead REQUIRE pairs of data layers for supervised?\n", "        yield from []\n", "        \n", "    def make_generator_validation(self) -> Generator[np.ndarray, None, None]:\n", "        \"\"\"Returns a generator yielding single samples of validation data. In the case of a training layer, this typically yields the model output.\"\"\"                \n", "        yield from []\n", "        \n", "    def make_generator_testing(self) -> Generator[np.ndarray, None, None]:\n", "        \"\"\"Returns a generator yielding single samples of testing data. In the case of a training layer, this typically yields the model output.\"\"\"                        \n", "        yield from []\n", "\n", "    @property\n", "    def accuracy_training(self) -> float:\n", "        \"\"\"Returns the current accuracy of the training phase\"\"\"        \n", "        return self._accuracy_training\n", "    \n", "    @property\n", "    def accuracy_validation(self) -> float:\n", "        \"\"\"Returns the current accuracy of the validation phase\"\"\"                \n", "        return self._accuracy_validation\n", "\n", "    @property\n", "    def accuracy_testing(self) -> float:\n", "        \"\"\"Returns the current accuracy of the testing phase\"\"\"                        \n", "        return self._accuracy_testing\n", "\n", "    @property\n", "    def loss_training(self) -> float:\n", "        \"\"\"Returns the current loss of the training phase\"\"\"                \n", "        return self._loss_training        \n", "\n", "    @property\n", "    def loss_validation(self) -> float:\n", "        \"\"\"Returns the current loss of the validation phase\"\"\"                        \n", "        return self._loss_validation        \n", "\n", "    @property\n", "    def loss_testing(self) -> float:\n", "        \"\"\"Returns the current loss of the testing phase\"\"\"                \n", "        return self._loss_testing\n", "\n", "    @property\n", "    def layer_weights(self) -> Dict[str, Dict[str, Picklable]]:\n", "        \"\"\"The weight values of each layer in the input Graph during the training.\n", "\n", "        Returns:\n", "            A dictionary of nested dictionaries, where each key is a layer id. The nested dictionaries contain weight name and value pairs. The values must be picklable.\n", "        \"\"\"        \n", "        return self._layer_weights\n", "\n", "    @property\n", "    def layer_biases(self) -> Dict[str, Dict[str, Picklable]]:\n", "        \"\"\"The bias values of each layer in the input Graph during the training.\n", "\n", "        Returns:\n", "            A dictionary of nested dictionaries, where each key is a layer id. The nested dictionaries contain weight name and value pairs. The values must be picklable.\n", "        \"\"\"        \n", "        return self._layer_biases\n", "    \n", "    @property\n", "    def layer_gradients(self) -> Dict[str, Dict[str, Picklable]]:\n", "        \"\"\"The gradients with respect to the loss of all trainable variables of each layer in the input Graph.\n", "\n", "        Returns:\n", "            A dictionary of nested dictionaries, where each key is a layer id. The nested dictionaries contain gradient name and value pairs. The values must be picklable.\n", "        \"\"\"        \n", "        return self._layer_gradients\n", "    \n", "    @property\n", "    def layer_outputs(self) -> Dict[str, Dict[str, Picklable]]:\n", "        \"\"\"The output values of each layer in the input Graph during the training (e.g., tf.Tensors evaluated for each iteration)\n", "\n", "        Returns:\n", "            A dictionary of nested dictionaries, where each key is a layer id. The nested dictionaries contain variable name and value pairs. The values must be picklable.\n", "        \"\"\"\n", "        return self._layer_outputs\n", "\n", "    @property\n", "    def training_iteration(self) -> int:\n", "        \"\"\"The current training iteration\"\"\"\n", "        return self._training_iteration\n", "\n", "    @property\n", "    def validation_iteration(self) -> int:\n", "        \"\"\"The current validation iteration\"\"\"        \n", "        return self._validation_iteration\n", "\n", "    @property\n", "    def testing_iteration(self) -> int:\n", "        \"\"\"The current testing iteration\"\"\"                \n", "        return self._testing_iteration\n", "    \n", "    @property\n", "    def progress(self) -> float:\n", "        \"\"\"A number indicating the overall progress of the training\n", "        \n", "        Returns:\n", "            A floating point number between 0 and 1\n", "        \"\"\"        \n", "        n_iterations_per_epoch = np.ceil(self.size_training / self.batch_size) + \\\n", "                                 np.ceil(self.size_validation / self.batch_size)\n", "        n_iterations_total = self._n_epochs * n_iterations_per_epoch\n", "\n", "        iteration = self.epoch * n_iterations_per_epoch + \\\n", "                    self.training_iteration + self.validation_iteration\n", "        \n", "        progress = min(iteration/(n_iterations_total - 1), 1.0) \n", "        return progress\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["layers = {\n", "    '_Data_1': DataData_Data_1(),\n", "    '_Reshape_1': ProcessReshape_Reshape_1(),\n", "    '_Data_2': DataData_Data_2(),\n", "    '_Convolution_1': DeepLearningConv_Convolution_1(),\n", "    '_OneHot_1': ProcessOneHot_OneHot_1(),\n", "    '_Fully_Connected_1': DeepLearningFC_Fully_Connected_1(),\n", "    '_Normal_1': TrainNormal_Normal_1(),\n", "}\n", "\n", "edges = {\n", "    ('_Data_1', '_Reshape_1'),\n", "    ('_Reshape_1', '_Convolution_1'),\n", "    ('_Data_2', '_OneHot_1'),\n", "    ('_Convolution_1', '_Fully_Connected_1'),\n", "    ('_OneHot_1', '_Normal_1'),\n", "    ('_Fully_Connected_1', '_Normal_1'),\n", "}\n", "\n", "graph_builder = GraphBuilder()\n", "graph = graph_builder.build(layers, edges)\n", "\n", "\n", "iterator = graph.training_nodes[0].layer_instance.run(graph)\n", "result = None\n", "sentinel = object()\n", "while result is not sentinel:\n", "    result = next(iterator, sentinel)\n", "\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.2"}}, "nbformat": 4, "nbformat_minor": 4}